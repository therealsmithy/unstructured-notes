---
title: "Topic Models"
format:
  html:
    toc: true
    toc-location: left
    self-contained: true
jupyter: python3
---

We already know that text is everywhere and our sentiment analysis was a reasonable crack at extracting some meaning from text. A common line of inquiry relates to what is expressed within the text. In traditionally social sciences, this was done through some form of content analysis -- a coding scheme was developed by researchers, people read each comment and assigned it a code, some agreement statistics were computed, and any discrepancies were hashed out by the researchers. When this was the only option, it was serviceable. 

The amount of information available to us now does not work for those traditional methods. Could you do this for thousands of tweets? With time, maybe. Could you do this with a few million articles? No. This is where topic models come to save the day. Our topic models are going to give us a pretty good idea about what texts are similar to other texts.

For the sake of exploration, it is great to know what topics are *latent* within a body of texts. It also has broader uses. When you search for something, topic models will return a set of documents that are likely to be related to that search. 

## Latent Dirichlet Allocation

Latent Semantic Analysis is the godfather of topic models and non-negative matrix factorization is a generally useful tool (it is great for working with images and has great extensions to factor analysis). For topic models, though, an important tool is latent dirichlet allocation (LDA).

Let's start with a brief demonstration of a standard latent dirichlet allocation (LDA) for topic modeling. A main point to take here is that the main driver of LDA is...the Dirichlet distribution. You can think of the Dirichlet distribution as a multivariate beta distribution (many possible categories, with probabilities of belonging to the category being between 0 and 1).

Suffice it to say, one can approach this in (at least) one of two ways. In one sense, LDA is a dimension reduction technique, much like the family of techniques that includes PCA, factor analysis, non-negative matrix factorization, etc. We will take a whole lot of terms, loosely defined, and boil them down to a few topics. In this sense, LDA is akin to discrete PCA. Another way to think about this is more from the perspective of factor analysis, where we are keenly interested in interpretation of the result, and want to know both what terms are associated with which topics, and what documents are more likely to present which topics. The following is the plate diagram and description for standard LDA from <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Blei, Jordan, and Ng (2003)</a>.

<aside>
Look at the citation count on that paper!
</aside>

![](bleiPlate.png)

- $\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions
- $\eta$ is the parameter of the Dirichlet prior on the per-topic word distribution
- $\theta_m$ is the topic distribution for document *m*
- $\beta_k$ is the word distribution for topic *k*
- $z_{mn}$ is the topic for the n-th word in document *m*
- $w_{mn}$ is the specific word

Both *z* and *w* are from a multinomial draw based on the $\theta$ and $\beta$ distributions respectively. The key idea is that, to produce a given document, one draws a topic, and given the topic, words are drawn from it.

Here is Blei's classic, short <a href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf">introduction</a> to probablistic topic models.

```{python}
#pip3 install bertopic
#pip3 install spacy
#pip3 install gensim
#pip3 install nltk
# bertopic is for transformers
from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer
# joblib is for saving and loading objects
from joblib import load, dump
import pandas as pd

# gensim is for LDA
import gensim
from gensim.models.coherencemodel import CoherenceModel
# nltk is for cleaning/prep
import nltk
# nltk.download('stopwords')
import pandas as pd
import pprint as pprint
# spacy is for cleaning/prep
import spacy
```

```{python}
songs = pd.read_feather(
  'G:/My Drive/MSBA/Unstructured/unstructured-notes/data/complete_lyrics_2025.feather'
)
songs = songs.dropna()

songs['lyrics'] = songs['lyrics'].astype('str')

songs['lyrics'] = songs['lyrics'].str.replace('([a-z])([A-Z])', '\\1 \\2', regex=True)

songs['lyrics'] = songs['lyrics'].str.replace('\\[.*?\\]', ' ', regex=True)

songs['lyrics'] = songs['lyrics'].str.replace('[Ee]mbeded|Embed', ' ', regex=True)

songs = songs[songs['lyrics'].str.contains("This song is an instrumental") == False]
```

```{python}
songs['word_count'] = songs['lyrics'].apply(lambda x: len(x.split()))

songs = songs[songs['word_count'] < 500]
```

```{python}
songs['genre'] = songs['week'].str.extract('((?<=charts/).*(?=/[0-9]{4}))')

songs['date'] = songs['week'].str.extract('([0-9]{4}-[0-9]{2}-[0-9]{2})')
```

We can start getting things prepped for analysis now. We will start by getting stopwords out and breaking out documents down into smaller pieces.

```{python}
stop_words = nltk.corpus.stopwords.words('english')

# deacc meants deaccent
def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc = True))  
        # yield is like return, but will return sequences

data_words = list(sent_to_words(songs['lyrics']))

def remove_stopwords(texts):
    return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]
```

```{python}
bigram = gensim.models.Phrases(
  # higher threshold fewer phrases.
  data_words, min_count=5, threshold=100) 

trigram = gensim.models.Phrases(
  bigram[data_words], threshold=100
)  

bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

nlp = nlp = spacy.load('en_core_web_lg')

def lemmatization(
  texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out
  
data_words_nostops = remove_stopwords(data_words)

# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)
```



```{python}
#| eval:false
data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

id2word = gensim.corpora.Dictionary(data_lemmatized)

texts = data_lemmatized

# Computationally intensive things should be saved down so you do not need to run it again
dump(
  [id2word, texts], 
  '/Users/sberry5/Documents/teaching/UDA/data/lda_data.joblib'
)
```

Now we are going to create a bag of words! This is a common way to represent text data. We will use the `doc2bow` method to create a bag of words for each document in our corpus. This will give us a list of tuples where the first element is the word id and the second element is the frequency of that word in the document.

```{python}
corpus = [id2word.doc2bow(text) for text in texts]
```

Now we can fit the model. We will use the `LdaModel` class from gensim to fit the model. We will use the `corpus` and `id2word` objects that we created earlier. We will also set the number of topics to 5 and the random state to 100. We will also set the `per_word_topics` parameter to `True` so that we can see the topic probabilities for each word in the document.

```{python}
lda_model = gensim.models.ldamodel.LdaModel(
  corpus=corpus,
  id2word=id2word,
  num_topics=5, # you pick
  random_state=100, # the seed
  update_every=1, # how often to update estimates
  chunksize=100, # how many docs in each training chunk
  passes=10, # how many rounds
  alpha='auto',
  per_word_topics=True 
)

dump(
  lda_model, 
  '/Users/sberry5/Documents/teaching/UDA/data/lda_model.joblib'
)
```

```{python}
pprint.pprint(lda_model.print_topics())
# Gives probability of song fitting into a topic
doc_lda = lda_model[corpus]
```

We want to see the topic probabilities for each document. We can do this by using the `get_document_topics` method. This will give us the topic probabilities for the first document in our corpus.
```{python}
# High level of love
lda_model.get_document_topics(corpus[0])
# High level of love and country
lda_model.get_document_topics(corpus[100])
```

We can also compute the perplexity and coherence of the model. This will give us an idea of how well the model is doing. The perplexity is a measure of how well the model predicts the data. The coherence is a measure of how well the topics are related to each other. In an ideal world, we would have a low perplexity and a high coherence. We would also try out a number of different topic models (k) to see which one is best.
```{python}
lda_model.log_perplexity(corpus)

coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')

coherence_lda = coherence_model_lda.get_coherence()

coherence_lda
```

## Transformers

We can also use transformers for topic modeling. This is a newer approach that is gaining popularity. We will use the `BERTopic` class from the `bertopic` package. We will also use the `ClassTfidfTransformer` class from the `bertopic.vectorizers` module. We will fit the model to our data and then save the model, topics, and probabilities to a file.

```{python}
# Clustered Tfidf
ctfidf_model = ClassTfidfTransforme(
  reduce_frequent_words=True
)
```

Notice that we can't use pandas objects directly, but have to convert them to lists.

```{python}
# BERt looks around the word for context. It can also fill in words.
topic_model = BERTopic(ctfidf_model=ctfidf_model)

topics, probs = topic_model.fit_transform(songs['lyrics'].to_list())

dump(
  [topic_model, topics, probs], 
  '/Users/sberry5/Documents/teaching/UDA/data/topic_model.joblib'
)
```

```{python}
topic_model, topics, probs = load(
  '/Users/sberry5/Documents/teaching/UDA/data/topic_model.joblib'
)
# BERTopic puts what it can't classify into topic -1
topic_model.get_topic_info()

topic_model.get_topic(0)

topic_model.get_document_info(songs['lyrics'])

topic_model.get_representative_docs(0)

topic_model.generate_topic_labels()

topic_model.reduce_topics(songs['lyrics'].to_list(), nr_topics=10)
```

You can also explore topics as they relate to specific classes within your data.

```{python}

docs = songs['lyrics'].to_list()
targets = songs['genre'].to_list()

topics_per_class = topic_model.topics_per_class(docs, classes=targets)
```

```{python}
topic_model.visualize_topics_per_class(topics_per_class, top_n_topics=10)

```

One of the best things you can do are dynamic topic models -- these allow you to see how topic expression can change over time.

```{python}
tops_over_time = topic_model.topics_over_time(
  songs['lyrics'].to_numpy(), 
  songs['date'].to_numpy(), 
  datetime_format="%Y-%M-%d", nr_bins=10
  )
```

```{python}
topic_model.visualize_topics_over_time(tops_over_time, top_n_topics=10)
```

```{python}
country_only = songs[songs['genre'] == 'country-songs']

topic_model_country = BERTopic(ctfidf_model=ctfidf_model)

topics, probs = topic_model_country.fit_transform(
  country_only['lyrics'].to_list())

dump(
  [topic_model_country, topics, probs], 
  '/Users/sberry5/Documents/teaching/UDA/data/topic_model_country.joblib'
)
```

```{python}
topic_model_country.reduce_topics(country_only['lyrics'].to_list(), nr_topics=10)
```

```{python}
country_over_time = topic_model_country.topics_over_time(
  country_only['lyrics'].to_numpy(), 
  country_only['date'].to_numpy(), 
  datetime_format="%Y-%M-%d", nr_bins=10
  )
```

```{python}
topic_model.visualize_topics_over_time(
  country_over_time, top_n_topics=10)
```